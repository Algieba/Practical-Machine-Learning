\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textbf{{#1}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Antonio Lloris Amor},
            pdftitle={Practical Machine Learning Project},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}
\setlength{\droptitle}{-2em}
  \title{Practical Machine Learning Project}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Antonio Lloris Amor}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \date{}
  \predate{}\postdate{}




\begin{document}

\maketitle


\section{Introduction}\label{introduction}

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now
possible to collect a large amount of data about personal activity to
predict the manner in which they did the exercise (``classe'' variable
in the training set). One thing that people regularly do is quantify how
much of a particular activity they do, but they rarely quantify how well
they do it.

In this project, our goal will be to use data from four sensors placed
in: belt, forearm, arm, and dumbell of 6 participants. They were asked
to perform barbell lifts correctly and incorrectly in 5 different ways.

In this report we are going to describe how we built our model, how we
used cross validation, what our expected out of sample error is, and why
we made the choices we did. We will also use our prediction model to
predict 20 different test cases.

\section{Preprocesing}\label{preprocesing}

\subsection{Data loading and columns
pruning}\label{data-loading-and-columns-pruning}

The fisrt step in our study is to load the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rawData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./OriginalData/pml-training.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our dataset has many columns but we are interested only in columns
related with ``roll'', ``pitch'', ``yaw'' and ``total\_accel'' from
sensors. Why this columns and not other, this columns are derivated from
x, y and z components from the ``magnet'', ``accel'' and ``gyros''
information of the sensors. There are other columns in original data set
but they are derivated from ``roll'', ``pitch'', ``yaw'' and
``total\_accel'' and these columns have many NA's.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prunedCol <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(rawData)[}\KeywordTok{grep}\NormalTok{(}\StringTok{"^roll_|^pitch_|^yaw_|^total_accel_"}\NormalTok{,}
                                    \KeywordTok{colnames}\NormalTok{(rawData))]}
\NormalTok{studyData <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(rawData, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"classe"}\NormalTok{,prunedCol))}
\KeywordTok{summary}\NormalTok{(studyData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  classe     roll_belt        pitch_belt          yaw_belt      
##  A:5580   Min.   :-28.90   Min.   :-55.8000   Min.   :-180.00  
##  B:3797   1st Qu.:  1.10   1st Qu.:  1.7600   1st Qu.: -88.30  
##  C:3422   Median :113.00   Median :  5.2800   Median : -13.00  
##  D:3216   Mean   : 64.41   Mean   :  0.3053   Mean   : -11.21  
##  E:3607   3rd Qu.:123.00   3rd Qu.: 14.9000   3rd Qu.:  12.90  
##           Max.   :162.00   Max.   : 60.3000   Max.   : 179.00  
##  total_accel_belt    roll_arm         pitch_arm          yaw_arm         
##  Min.   : 0.00    Min.   :-180.00   Min.   :-88.800   Min.   :-180.0000  
##  1st Qu.: 3.00    1st Qu.: -31.77   1st Qu.:-25.900   1st Qu.: -43.1000  
##  Median :17.00    Median :   0.00   Median :  0.000   Median :   0.0000  
##  Mean   :11.31    Mean   :  17.83   Mean   : -4.612   Mean   :  -0.6188  
##  3rd Qu.:18.00    3rd Qu.:  77.30   3rd Qu.: 11.200   3rd Qu.:  45.8750  
##  Max.   :29.00    Max.   : 180.00   Max.   : 88.500   Max.   : 180.0000  
##  total_accel_arm roll_dumbbell     pitch_dumbbell     yaw_dumbbell     
##  Min.   : 1.00   Min.   :-153.71   Min.   :-149.59   Min.   :-150.871  
##  1st Qu.:17.00   1st Qu.: -18.49   1st Qu.: -40.89   1st Qu.: -77.644  
##  Median :27.00   Median :  48.17   Median : -20.96   Median :  -3.324  
##  Mean   :25.51   Mean   :  23.84   Mean   : -10.78   Mean   :   1.674  
##  3rd Qu.:33.00   3rd Qu.:  67.61   3rd Qu.:  17.50   3rd Qu.:  79.643  
##  Max.   :66.00   Max.   : 153.55   Max.   : 149.40   Max.   : 154.952  
##  total_accel_dumbbell  roll_forearm       pitch_forearm   
##  Min.   : 0.00        Min.   :-180.0000   Min.   :-72.50  
##  1st Qu.: 4.00        1st Qu.:  -0.7375   1st Qu.:  0.00  
##  Median :10.00        Median :  21.7000   Median :  9.24  
##  Mean   :13.72        Mean   :  33.8265   Mean   : 10.71  
##  3rd Qu.:19.00        3rd Qu.: 140.0000   3rd Qu.: 28.40  
##  Max.   :58.00        Max.   : 180.0000   Max.   : 89.80  
##   yaw_forearm      total_accel_forearm
##  Min.   :-180.00   Min.   :  0.00     
##  1st Qu.: -68.60   1st Qu.: 29.00     
##  Median :   0.00   Median : 36.00     
##  Mean   :  19.21   Mean   : 34.72     
##  3rd Qu.: 110.00   3rd Qu.: 41.00     
##  Max.   : 180.00   Max.   :108.00
\end{verbatim}

The columns used in our study are: classe, roll\_belt, pitch\_belt,
yaw\_belt, total\_accel\_belt, roll\_arm, pitch\_arm, yaw\_arm,
total\_accel\_arm, roll\_dumbbell, pitch\_dumbbell, yaw\_dumbbell,
total\_accel\_dumbbell, roll\_forearm, pitch\_forearm, yaw\_forearm,
total\_accel\_forearm

\subsection{Get training and test
sets}\label{get-training-and-test-sets}

For cross validation I going to divide it in two portions with the same
size. Each portion will be divided in a training and a test set (60\% /
40\%).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(lattice)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(corrplot)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{portions <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =} \NormalTok{studyData$classe,}
                                \DataTypeTok{p =} \FloatTok{0.5}\NormalTok{,}
                                \DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{Data1 <-}\StringTok{ }\NormalTok{studyData[portions,]}
\NormalTok{Data2 <-}\StringTok{ }\NormalTok{studyData[-portions,]}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{inTrain <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =} \NormalTok{Data1$classe,}
                               \DataTypeTok{p =} \FloatTok{0.6}\NormalTok{,}
                               \DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{trainingData1 <-}\StringTok{ }\NormalTok{Data1[inTrain,]}
\NormalTok{testingData1 <-}\StringTok{ }\NormalTok{Data1[-inTrain,]}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{inTrain <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(}\DataTypeTok{y =} \NormalTok{Data2$classe,}
                               \DataTypeTok{p =} \FloatTok{0.6}\NormalTok{,}
                               \DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{trainingData2 <-}\StringTok{ }\NormalTok{Data2[inTrain,]}
\NormalTok{testingData2 <-}\StringTok{ }\NormalTok{Data2[-inTrain,]}
\end{Highlighting}
\end{Shaded}

We have two traning sets with 5.889 and 5.887 and two test sets with
3.923 and 3.923 observations.

\subsection{Plotting predictors}\label{plotting-predictors}

In the following graph we can see that there isn't a unique predictor
that can classify the training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainingDataU <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(trainingData1,trainingData2)}

\KeywordTok{featurePlot} \NormalTok{(}\DataTypeTok{x =} \NormalTok{trainingDataU[,-}\DecValTok{1}\NormalTok{], }\DataTypeTok{y =} \NormalTok{trainingDataU$classe,}
             \DataTypeTok{plot =} \StringTok{"box"}\NormalTok{,}
             \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"classe"}\NormalTok{, }\StringTok{"acceleration"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{PracMachineLearning_PeerAssessment1_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(trainingDataU[,-}\DecValTok{1}\NormalTok{]), }\DataTypeTok{method =} \StringTok{"color"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{PracMachineLearning_PeerAssessment1_files/figure-latex/unnamed-chunk-5-1.pdf}
The previous graph show us that there are correlations between the
predictors.

\section{Prediction}\label{prediction}

Now I going to use the \(traininData1\) and \(traininData2\) datasets
for training process with different method. The training process include
a cross validation step.

\subsection{Classification Tree}\label{classification-tree}

\subsubsection{Training}\label{training}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{modelFit1 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe ~., }\DataTypeTok{data =} \NormalTok{trainingData1,}
                  \DataTypeTok{method =} \StringTok{"rpart"}\NormalTok{,}
                  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{4}\NormalTok{))}
\NormalTok{modelFit1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CART 
## 
## 5889 samples
##   16 predictor
##    5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 4416, 4417, 4416, 4418 
## 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa       Accuracy SD  Kappa SD  
##   0.03368921  0.5158661  0.38281804  0.02493363   0.03395737
##   0.04329775  0.4179267  0.21773200  0.06178628   0.11198375
##   0.11720047  0.3233221  0.05970543  0.04521353   0.06904997
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03368921.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{modelFit2 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe ~., }\DataTypeTok{data =} \NormalTok{trainingData2,}
                  \DataTypeTok{method =} \StringTok{"rpart"}\NormalTok{,}
                  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{4}\NormalTok{))}
\NormalTok{modelFit2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CART 
## 
## 5887 samples
##   16 predictor
##    5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 4414, 4417, 4414, 4416 
## 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa       Accuracy SD  Kappa SD  
##   0.04035129  0.4223369  0.22308721  0.06733448   0.11678891
##   0.05174460  0.3949745  0.17469238  0.06261363   0.10694905
##   0.11606931  0.3220600  0.05774784  0.04353458   0.06674853
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.04035129.
\end{verbatim}

We can see that the values for \(Accuracy\) are very pour.

\subsubsection{Testing}\label{testing}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{predictions1 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelFit1, }\DataTypeTok{newdata=}\NormalTok{testingData1)}
\KeywordTok{confusionMatrix}\NormalTok{(predictions1, testingData1$classe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 798 272 146 179  99
##          B  59 289  42 126 174
##          C 186 166 469 165 114
##          D  70  32  27 173  28
##          E   3   0   0   0 306
## 
## Overall Statistics
##                                          
##                Accuracy : 0.5187         
##                  95% CI : (0.503, 0.5345)
##     No Information Rate : 0.2845         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.3834         
##  Mcnemar's Test P-Value : < 2.2e-16      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.7151  0.38076   0.6857  0.26905  0.42441
## Specificity            0.7520  0.87326   0.8052  0.95213  0.99906
## Pos Pred Value         0.5341  0.41884   0.4264  0.52424  0.99029
## Neg Pred Value         0.8691  0.85462   0.9238  0.86919  0.88517
## Prevalence             0.2845  0.19347   0.1744  0.16391  0.18379
## Detection Rate         0.2034  0.07367   0.1196  0.04410  0.07800
## Detection Prevalence   0.3808  0.17589   0.2804  0.08412  0.07877
## Balanced Accuracy      0.7336  0.62701   0.7454  0.61059  0.71174
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{predictions2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelFit2, }\DataTypeTok{newdata=}\NormalTok{testingData2)}
\KeywordTok{confusionMatrix}\NormalTok{(predictions2, testingData2$classe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 959 350 305 225  78
##          B  65 215  30 178  42
##          C  87 194 349 240 265
##          D   0   0   0   0   0
##          E   5   0   0   0 336
## 
## Overall Statistics
##                                           
##                Accuracy : 0.4739          
##                  95% CI : (0.4581, 0.4896)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.3153          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.8593   0.2833  0.51023   0.0000  0.46602
## Specificity            0.6587   0.9004  0.75733   1.0000  0.99844
## Pos Pred Value         0.5003   0.4057  0.30749      NaN  0.98534
## Neg Pred Value         0.9217   0.8397  0.87984   0.8361  0.89252
## Prevalence             0.2845   0.1935  0.17436   0.1639  0.18379
## Detection Rate         0.2445   0.0548  0.08896   0.0000  0.08565
## Detection Prevalence   0.4887   0.1351  0.28932   0.0000  0.08692
## Balanced Accuracy      0.7590   0.5919  0.63378   0.5000  0.73223
\end{verbatim}

\subsection{Random forest}\label{random-forest}

\subsubsection{Training}\label{training-1}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{modelFit3 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe ~., }\DataTypeTok{data =} \NormalTok{trainingData1,}
                  \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{4}\NormalTok{))}
\NormalTok{modelFit3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 5889 samples
##   16 predictor
##    5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 4416, 4417, 4416, 4418 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9653611  0.9561781  0.007325281  0.009272088
##    9    0.9702842  0.9624194  0.003851995  0.004872157
##   16    0.9631516  0.9534091  0.003975251  0.005016614
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 9.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{modelFit4 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(classe ~., }\DataTypeTok{data =} \NormalTok{trainingData2,}
                  \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                  \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{4}\NormalTok{))}
\NormalTok{modelFit4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 5887 samples
##   16 predictor
##    5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 4414, 4417, 4414, 4416 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9706062  0.9628218  0.009204451  0.011637822
##    9    0.9695911  0.9615365  0.004046331  0.005121274
##   16    0.9619481  0.9518811  0.003403157  0.004312700
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.
\end{verbatim}

We can see that the values for \(Accuracy\) are very pour.

\subsubsection{Testing}\label{testing-1}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{predictions3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelFit3, }\DataTypeTok{newdata=}\NormalTok{testingData1)}
\KeywordTok{confusionMatrix}\NormalTok{(predictions3, testingData1$classe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1103   12    1    0    3
##          B    6  723   11    3    9
##          C    0   21  663   11   10
##          D    4    3    9  627    2
##          E    3    0    0    2  697
## 
## Overall Statistics
##                                           
##                Accuracy : 0.972           
##                  95% CI : (0.9663, 0.9769)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9645          
##  Mcnemar's Test P-Value : 0.001104        
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9884   0.9526   0.9693   0.9751   0.9667
## Specificity            0.9943   0.9908   0.9870   0.9945   0.9984
## Pos Pred Value         0.9857   0.9614   0.9404   0.9721   0.9929
## Neg Pred Value         0.9954   0.9886   0.9935   0.9951   0.9925
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2812   0.1843   0.1690   0.1598   0.1777
## Detection Prevalence   0.2852   0.1917   0.1797   0.1644   0.1789
## Balanced Accuracy      0.9913   0.9717   0.9782   0.9848   0.9826
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{predictions4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelFit4, }\DataTypeTok{newdata=}\NormalTok{testingData2)}
\KeywordTok{confusionMatrix}\NormalTok{(predictions4, testingData2$classe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1107   30    0    1    1
##          B    3  717    9    1    8
##          C    5   10  668   13    2
##          D    0    2    7  626    3
##          E    1    0    0    2  707
## 
## Overall Statistics
##                                           
##                Accuracy : 0.975           
##                  95% CI : (0.9696, 0.9797)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9684          
##  Mcnemar's Test P-Value : 1.396e-05       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9919   0.9447   0.9766   0.9736   0.9806
## Specificity            0.9886   0.9934   0.9907   0.9963   0.9991
## Pos Pred Value         0.9719   0.9715   0.9570   0.9812   0.9958
## Neg Pred Value         0.9968   0.9868   0.9950   0.9948   0.9956
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2822   0.1828   0.1703   0.1596   0.1802
## Detection Prevalence   0.2903   0.1881   0.1779   0.1626   0.1810
## Balanced Accuracy      0.9903   0.9690   0.9837   0.9850   0.9898
\end{verbatim}

\subsection{In/Out sample error}\label{inout-sample-error}

\subsubsection{In sample error}\label{in-sample-error}

\begin{longtable}[c]{@{}lrrr@{}}
\toprule
Method & Training Set \#1 & Training Set \#2 & Mean\tabularnewline
\midrule
\endhead
Classification Tree & 0.4841339 & 0.5776631 & 0.5308985\tabularnewline
Random forest & 0.0297158 & 0.0293938 & 0.0295548\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Out sample error}\label{out-sample-error}

\begin{longtable}[c]{@{}lrrr@{}}
\toprule
Method & Testing Set \#1 & Testing Set \#2 & Mean\tabularnewline
\midrule
\endhead
Classification Tree & 0.4813 & 0.5261 & 0.5037\tabularnewline
Random forest & 0.028 & 0.025 & 0.0265\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Conclusion}\label{conclusion}

Out sample error \textless{} In sample error, our model don't have
overfitting.

\section{Validation}\label{validation}

Now we are goint to use \("pml-testing.csv"\) file for validate our
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rawData <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./OriginalData/pml-testing.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prunedCol <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(rawData)[}\KeywordTok{grep}\NormalTok{(}\StringTok{"^roll_|^pitch_|^yaw_|^total_accel_"}\NormalTok{,}
                                    \KeywordTok{colnames}\NormalTok{(rawData))]}
\NormalTok{validationData <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(rawData, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(prunedCol))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{260668}\NormalTok{)}

\NormalTok{predictions4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelFit4, }\DataTypeTok{newdata=}\NormalTok{validationData)}
\NormalTok{predictions4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
\end{verbatim}

\end{document}
